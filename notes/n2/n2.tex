\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[overload]{empheq}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage[hang,flushmargin]{footmisc}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\tit}{\textit}
\newcommand{\tbf}{\textbf}
\newcommand{\ran}{\text{ran}}
\newcommand{\Img}{\text{Im}}
%\newcommand{\ker}{\text{ker}}
\DeclareMathOperator*{\argmin}{arg\,min}


% These two lines are from this StackExchange post: https://tex.stackexchange.com/a/177270
\usepackage{sectsty}
\allsectionsfont{\mdseries}

% The following, up to \title, is from this StackOverflow post: https://stackoverflow.com/a/3175141
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{Note 2: Review of Basic Linear Algebra}
\author{Math 198: Math for Machine Learning}
\date{}

\begin{document}
\maketitle

\section{Topic: Vector Spaces, Linear Maps, and Matrices}
Linear algebra touches nearly every facet of machine learning. Broadly, linear algebra is the study of \textit{vector spaces} and the maps between them, \textit{linear transformations}. 

\subsection{Vector Spaces and Subspaces}
A (real) \textit{vector space} is a set $V$ that is closed under finite vector addition and scalar multiplication and that satisfies the following axioms:
\begin{enumerate}[label=(\alph*)]
\item Associativity of addition: $(\tbf{x}+\tbf{y})+\tbf{z} = \tbf{x}+(\tbf{y}+\tbf{z})$;
\item Additive identity: There exists an identity element $\tbf{0} \in V$ such that $\tbf{x}+\tbf{0} = \tbf{x}$ for all $\tbf{x} \in V$;
\item Additive inverses: For every $\tbf{x} \in V$ there exists an element $-\tbf{x}$ such that $\tbf{x} + (-\tbf{x}) = \tbf{0}$;
\item Commutativity of addition\footnote{Note that axioms (a) through (d) say that $(V, +)$ is an abelian group.}: $\tbf{x}+\tbf{y} = \tbf{y}+\tbf{x}$;
\item Associativity of scalar multiplication: $a(b\tbf{x}) = (ab)\tbf{x}$;
\item Distributivity: $a(\tbf{x}+\tbf{y}) = a\tbf{x} + a\tbf{y}$ and $(a+b)\tbf{x} = a\tbf{x} + b\tbf{x}$;
\item Multiplicative identity: $1\tbf{x} = \tbf{x}$, where $1 \in \mathbb{R}$.
\end{enumerate}
By convention, we refer to the vector space as $V$ and to an element of $V$ as a \tit{vector}. Some vector spaces we'll be working with are
\begin{itemize}
\item $\mathbb{R}$ and $\mathbb{R}^d$, the spaces of one- or $d$-dimensional vectors over the real numbers
\item $\mathbb{R}^{m \times n}$, the space of $m\times n$ matrices with real entries
\item $\mathbb{P}_n$, the space of $n^{\text{th}}$-degree polynomials on $\R$ with real coefficients.
\end{itemize}
A \tit{subspace} of a vector space $V$ is a subset $U \subseteq V$ such that $U$ is a vector space under the same addition and scalar multiplication operations. Subspaces are easy to characterize: A nonempty subset $U \subseteq V$ is a subspace iff $U$ contains $\tbf{0}$ and is closed under addition and scalar multiplication. No need to check the other axioms -- since they are met in $V$, they are met in $U$.

\subsection{Basis and Dimension}
We'll quickly run through some key definitions. Let $V$ be a vector space. Given $\tbf{x}_1, ..., \tbf{x}_k \in V$, a \tit{linear combination} of $\tbf{x}_1, ..., \tbf{x}_k$ is any vector of the form $a_1\tbf{x}_1 + ... + a_k\tbf{x}_k$, where $a_i \in \R$. Note that saying $V$ is closed under finite addition and scalar multiplication is equivalent to saying that $V$ is closed under taking linear combinations. Given some subset $A \subseteq V$, define the \tit{span} of $A$, denoted $\text{span}(A)$, to be the set of linear combinations of vectors in $A$.  A nonzero set of vectors $\{\tbf{x}_1, ... , \tbf{x}_k\} \subseteq V$ is said to be \tit{linearly independent} if there do not exist scalars $a_1, ... ,a_k$, all nonzero, such that $a_1\tbf{x}_1 + ... + a_k\tbf{x}_k = \mathbf{0}$, i.e. you can't write any of the vectors as a nontrivial linear combination of the others. The definition implies that any set of vectors containing $\tbf{0}$ is not linearly independent.
\\ \\
A \tit{basis} for $V$ is a set $B = \{\tbf{x}_1, ... , \tbf{x}_d\} \subseteq V$ such that (i) $B$ is linearly independent and (ii) $\text{span}(B) = V$. Intuitively, (i) ensures that $B$ doesn't have too many vectors, and (ii) ensures that $B$ has enough vectors to write every $\tbf{x} \in V$ as a linear combination of vectors in $B$. Some facts about bases: 
\begin{itemize}
\item Does every vector space have a basis? Yes, if we assume Zorn's lemma\footnote{Zorn's lemma $\iff$ Axiom of Choice $\iff$ every vector space has a basis. When the existence of something is shown with Zorn's lemma, it is often difficult to construct an example of it. Can you exhibit a basis for $\R$ as a vector space over $\Q$?} holds. 
\item Bases are not unique: You can check that $\{(1,0), (0,1)\}$ and $\{(1,1), (1,-1)\}$ both form bases for $\R^2$.
\item Given a subset $S \subset V$, $S$ could fail to be a basis because it has too many vectors (i.e. it's not linearly independent), it doesn't have enough vectors (i.e. it doesn't span $V$), or a combination of the two (too few and linearly dependent). But these problems are easy to fix: we can always create a basis from $S$ by adding vectors until $S$ spans $V$ and/or removing vectors until $S$ is linearly independent. 
\item Most importantly: Every basis of $V$ has the same number of vectors; this number is known as the \tit{dimension} of $V$, denoted $\dim{V}$. Dimension is unique.
\end{itemize}
We will work almost exclusively with finite-dimensional vector spaces\footnote{Infinite dimensional vector spaces are usually spaces of functions, e.g. $C(\R, \R)$, the space of continuous functions from $\R$ to $\R$ with pointwise addition and s.m.. Their study is known as \tit{functional analysis}.}. The standard basis for the $d$-dimensional vector space $\R^d$ is $\{e_1, ... , e_d\}$, where $e_1 = (1,0,...,0)$, $e_2 = (0,1,0,...,0)$, etc.  From now on, assume that every vector space is finite-dimensional unless stated otherwise.

\subsection{Inner Products, Orthogonality, and Norms}
For a real vector space $V$, an \tit{inner product} is a map $\la \cdot, \cdot \ra: V \times V \to \mathbb{R}$ satisfying 
\begin{enumerate}[label = (\alph*)]
\item Linearity in the first coordinate: $\la a\tbf{x} + \tbf{y}, \tbf{z} \ra = a\la \tbf{x},\tbf{z}\ra + \la \tbf{y}, \tbf{z} \ra$
\item Symmetry: $\la \tbf{x},\tbf{y}\ra = \la \tbf{y},\tbf{x}\ra$
\item Positive semi-definite: $\forall v \in V, \la v, v \ra \geq 0$; $\la v, v \ra = 0 \iff v = \mathbf{0}$
\end{enumerate}
By symmetry, the inner product is linear in both coordinates. A vector space equipped with an inner product is called an \tit{inner product space}. Note that an inner product induces a \tit{norm} (size) on $V$ given by $||\tbf{x}|| = \sqrt{\langle \bold{x},  \bold{x}\rangle}$, which in turn induces a \tit{metric} (distance) on $V$ given by $d(\tbf{x},\tbf{y}) = ||\tbf{x} - \tbf{y}||$. For our purposes, we will make use of the standard inner product on $\mathbb{R}^d$, the dot product: 
$$ \langle \bold{x},  \bold{y}\rangle = \sum_{i=1}^d x_iy_i.$$ 
Inner products allow us to define the notion of orthogonality. Two vectors $\tbf{x}, \tbf{y}$ are \tit{orthogonal} if $\la \tbf{x}, \tbf{y} \ra = 0$. This will be important when we cover linear approximation.
\\ \\
Norms allow us to assign a ``size" to each vector. In $\R^d$, there is an important family of norms called the $\ell^p$-norms (a.k.a. $p$-norms). For $p \in \Z, p \geq 1$, define 
$$||\bold{x}||_p = \left(\sum_{i = 1}^d (x_i)^p\right)^\frac{1}{p}.$$
Note that $||\bold{x}||_2 = \sqrt{\langle \bold{x},  \bold{x}\rangle}$, the norm induced by the dot product.

\subsection{Linear Maps and Isomorphism}
Let $V,W$ be vector spaces. A function $T: V \to W$ is said to be a \tit{linear map} if 
$$T(a\tbf{x} + \tbf{y}) = aT(\tbf{x}) + T(\tbf{y})$$
That is, a linear map preserves vector addition and scalar multiplication.\\ \\
Associated with $T$ are two important subspaces, the range and the kernel. The range (a.k.a. the image) of $T$, denoted $\text{ran}(T)$ or $\text{Im}(T)$, is given by $\text{ran}(T) = \{ \tbf{y} \in W: \tbf{y} = T(\tbf{x}) \text{ for some } \tbf{x} \in V \}$. The kernel of $T$, denoted $\ker(T)$, is given by $\ker(T) = \{\tbf{x} \in V: T(\tbf{x}) = \tbf{0}_W\}$. The image of $T$ is a subspace of $W$, and the kernel of $T$ is a subspace of $V$. An important result, the Rank-Nullity theorem, states that $\dim(\text{ran}(T)) + \dim(\text{ker}(T)) = \dim(V)$.
\\ \\
The linearity of linear maps makes them interact nicely with the structure of the vector spaces involved. An important property of linear maps is that their behavior is determined completely be their action on a basis for the domain. Let $T: V \to W$ be a linear map and $B = \{\tbf{x}_1, ... , \tbf{x}_d\}$ a basis for $V$. Suppose that we know $T(\tbf{x}_i)$ for all $i = 1, ... , d$. Choose some arbitrary vector $\tbf{v} \in V$. Since $B$ is a basis, we can write $\tbf{v}$ as a linear combination $\tbf{v} = a_1\tbf{x}_1 + ... + a_d\tbf{x}_d$. Then, by linearity, $T(\tbf{v}) = a_1T(\tbf{x}_1) + ... + a_dT(\tbf{x}_d)$.
\\ \\
How can we say that two arbitrary vector spaces are ``the same"? We use the notion of an \tit{isomorphism}. The following are equivalent statements about a linear map $T: V \to W$:
\begin{enumerate}[label=(\roman*)]
\item $T$ is one-to-one and onto
\item $T$ is an isomorphism
\item $T$ has an inverse, $T^{-1}$ 
\item $V,W$ have the same dimension and $\ker(T) = \tbf{0}_V$ (i.e. $T$ is one-to-one)
\item $V,W$ have the same dimension and $\ran(T) = W$ (i.e. $T$ is onto)
\item Applying $T$ to each element of a basis for $V$ results in a basis for $W$
\end{enumerate}
The vector spaces $V$ and $W$ are \tit{isomorphic} if there exists an isomorphism between them, in which case we write $V \cong W$. The above equivalences imply that two vector spaces are isomorphic if and only if they share the same dimension. Thus, every $d$-dimensional vector space is isomorphic to $\R^d$. Given a $d$-dimensional vector space $V$, how do we exhibit such an isomorphism to $\R^d$? By choosing a basis $B = \{\tbf{x}_1, ... , \tbf{x}_d\}$ and letting $T(\tbf{x}_i) = e_i$. If $\dim V = n$ and $\dim W = m$, we can always identify $V$ with $\R^n$ and $W$ with $\R^m$ if needed. 

\subsection{Matrices}
The key idea of this section is that we can concretely represent linear maps between finite-dimensional vector spaces as matrices. Given a map $T: \R^n \to \R^m$, we can form a matrix $\tbf{A} \in \R^{m\times n}$ such that $\tbf{A}\tbf{x} = T(\tbf{x})$ by setting the $i^{\text{th}}$ column of $A$ to be the column vector $T(\tbf{e}_i) \in \R^m$. In other words, 
$$
\tbf{A} = 
\begin{bmatrix}
T(\tbf{e}_1) & \dots & T(\tbf{e}_n) 
\end{bmatrix}.
$$
To see that this construction works, recall that the action of $T$ is
 $$
 \tbf{x} = x_1\tbf{e}_1 + ... + x_n \tbf{e}_n \mapsto T(\tbf{x}) = x_1T(\tbf{e}_1) + ... + x_n T(\tbf{e}_n) = \tbf{A}\tbf{x}.
 $$
Note that we necessarily define $A$ with respect to an ordered basis for $\R^n$ and $\R^m$. In fact, any time we write out the elements of a vector or matrix, we do so with respect to some ordered basis. For example, the $i$-th column of a matrix $A$ represents the action of that matrix on the $i$-th basis vector. To limit confusion, you can always assume that a matrix or vector is being written with respect to the standard bases unless otherwise noted.

\clearpage
\section{Applications: Projections}
\subsection{Motivation}
Recall from note 1 that the goal of Ordinary Least Squares is to determine a weight vector $\mathbf{w}$ such that $\mathbf{Xw} \approx \mathbf{y}$ for our data matrix $\mathbf{X}$ and observations $y$. Suppose that $y \in \text{range}(\mathbf{X})$ and that $\mathbf{X}$ is invertible.\footnote{Since a matrix $\mathbf{A}$ represents a linear map $T$, $A$ is invertible if $T$ is invertible, and the inverse of $\mathbf{A}$, $\mathbf{A^{-1}}$, represents $T^{-1}$ with respect to the same bases as $\mathbf{A}$.} Then we could solve directly: $\mathbf{w} = \mathbf{X}^{-1}\mathbf{y}$. Of course, this scenario is rarely, if ever, seen in practice. In general, we will not be able to come up with an exact solution for the equation $\mathbf{Xw} = \mathbf{y}$; instead, we seek a weight vector $\mathbf{w}$ such that $\mathbf{Xw}$ is the best approximation to $\mathbf{y}$ in the range of $\mathbf{X}$.\footnote{We will not yet present a probabilistic motivation for our idea of "closeness"; this will be done in the probability section of the course.} To do so, we will first introduce the notion of an \tit{orthogonal projection}.

\subsection{Projectors}
Suppose we have some vector space $V$, some subspace $W \subset V$, and some element $\mathbf{v} \in V$ such that $\mathbf{v} \notin W$. Define the orthogonal projection of $\mathbf{v}$ in $W$, $\mathbf{v}_w$, to be the vector in $W$ which is closest to $\mathbf{v}$: $$\mathbf{v}_w = \argmin\limits_{\mathbf{w} \in W} ||\mathbf{v} - \mathbf{w}||$$ How can we go about finding such a vector? The first step is to note that $\mathbf{v}_w$ is the closest vector in $W$ to $\mathbf{v}$ if and only if $\mathbf{v} - \mathbf{v}_w$ is orthogonal to every $\mathbf{w} \in W$. \\\\
\tit{Proof}. Fix some arbitrary $\mathbf{w} \in W$, and define the function\footnote{Do not confuse the $w$ in $\mathbf{v}_w$ and the $\mathbf{w}$ in the $t\mathbf{w}$ term; $\mathbf{v}_w$ is the orthogonal projection of $\mathbf{v}$ in $W$, and $\mathbf{w}$ is some arbitrary vector in $W$.} $f_w(t) = ||\mathbf{v} - (\mathbf{v}_w + t\mathbf{w})||^2$. Then $f$ is the square of the distance between $\mathbf{v}_w + t\mathbf{w}$, a vector in $W$, and $v$. It should be clear that $f$ is minimized when $t = 0$. So, the derivative of $f_w$ at $t = 0$ is 0. To determine the derivative of $f_w$, we first expand it by rewriting it as an inner product: $$f_w(t) = \la (\mathbf{v} - \mathbf{v}_w) - t\mathbf{w}, (\mathbf{v} - \mathbf{v}_w) - t\mathbf{w} \ra$$ $$= \la \mathbf{v} - \mathbf{v}_w, \mathbf{v} - \mathbf{v}_w \ra - 2\la \mathbf{v} - \mathbf{v}_w, t\mathbf{w} \ra + \la t\mathbf{w}, t\mathbf{w}\ra $$ $$= ||\mathbf{v} - \mathbf{v}_w||^2 - 2t\la \mathbf{v} - \mathbf{v}_w, \mathbf{w}\ra + t^2||\mathbf{w}||^2$$ We then take the derivative with respect to $t$: $$f'_w(t) = -2\la \mathbf{v} - \mathbf{v}_w, \mathbf{w}\ra + 2t||\mathbf{w}||^2$$ and so $$0 = f'_w(0) = -2\la \mathbf{v} - \mathbf{v}_w, \mathbf{w} \ra$$ and so $\mathbf{v} - \mathbf{v}_w$ is orthogonal to $\mathbf{w}$. Since our choice of $\mathbf{w}$ was arbitrary, we conclude that $\mathbf{v} - \mathbf{v}_w$ is orthogonal to every vector in $W$. To prove the converse, note that $f_w$ is quadratic in its input and non-negative; so if $\mathbf{v} - \mathbf{v}_w$ is orthogonal to every vector in $W$, then $f'_w(0) = 0$, and so $t = 0$ must be the global minimum of $f_w$ for all $\mathbf{w}$; so, $||\mathbf{v} - (\mathbf{v}_w + t\mathbf{w})||^2$ is minimized for $t = 0$, and thus $\mathbf{v}_w$ is the closest vector in W to $\mathbf{v}$. $\hfill\square$ \\\\
This proof has an important corollary. We have proven that $\mathbf{v} - \mathbf{v}_w$ is orthogonal to the subspace $W$. Let $W^{\top}$ denote the set of all vectors in $V$ which are orthogonal to $W$; so, $\mathbf{v} - \mathbf{v}_w \in W^{\top}$. It turns out that $W^{\top}$ is itself a subspace such that $W \oplus W^{\top} = V$. \\\\
\tit{Proof}. We first show that $W^{\top}$ is a subspace of $V$. Fix $\mathbf{v_1}, \mathbf{v_2} \in W^{\top}$ and $a \in \R$. Then, for all $\mathbf{w} \in W$, $$\la \mathbf{v_1} + \mathbf{v_2}, \mathbf{w}\ra = \la \mathbf{v_1}, \mathbf{w}\ra + \la \mathbf{v_2}, \mathbf{w} \ra = 0 + 0 = 0$$ $$\la a\mathbf{v_1}, \mathbf{w} \ra = a\la \mathbf{v_1}, \mathbf{w} \ra = a0 = 0$$ so $\mathbf{v_1} + \mathbf{v_2} \in W^{\top}$ and $a\mathbf{v_1} \in W^{\top}$ and so $W^{\top}$ is a subspace. We now show that $W \oplus W^{\top} = V$. To do so, we show that any vector $\mathbf{v} \in V$ can be decomposed into the sum of two vectors, one in $W$, and one in $W^{\top}$. Of course, $\mathbf{v} = \mathbf{v}_w + (\mathbf{v} - \mathbf{v}_w)$; since $\mathbf{v}_w \in W$ and $\mathbf{v} - \mathbf{v}_w \in W^{\top}$, we have $V = W \oplus W^{\top}$. $\hfill\square$ \\\\
Suppose $V$ has dimension $n$ and $W$ has dimension $k$. Then by the corollary, $W^{\top}$ has dimension $n - k$. Furthermore, suppose we have some orthogonal\footnote{Such a basis is guaranteed to exist. We can turn any basis into an orthonormal basis using Gram-Schmidt Orthonormalization, which is out of scope for this course because it is computationally horrifying. So, we can take any basis for $W$, orthonormalize it, and then extend it to an orthonormal basis for $V$ to obtain the desired basis for any finite vector space.} basis (likely non-standard) for $V$, $\beta = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$, such that $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ is a basis for $W$. Then $\{\mathbf{v}_{k+1}, \ldots, \mathbf{v}_n\}$ is a basis for $W^{\top}$.\footnote{The proof for this is trivial, but the fact that our basis for $V$ is orthogonal is essential.} Consider some vector $\mathbf{v} \in V$. We can write $\mathbf{v} = \sum\limits_{i = 1}^n \alpha_i\mathbf{v}_i$ for appropriate coefficients $\alpha_i$. Define $\mathbf{v}_w = \sum\limits_{i = 1}^k\alpha_i\mathbf{v}_i$. Then $$\mathbf{v} - \mathbf{v}_w = \sum\limits_{i = k+1}^n\alpha_i\mathbf{v}_i \in W^{\top}$$ and so $\mathbf{v}_w = \sum\limits_{i=1}^k\alpha_i\mathbf{v}_i$ is the orthogonal projection of $\mathbf{v}$ in $W$. So we have reduced the problem of finding the closest approximation to $\mathbf{v}$ to the problem of finding an orthogonal basis for our subspace, $W$.

\subsection{Conclusion}
In section 2.1, we concluded that we seek a weight vector $\mathbf{w}$ such that $\mathbf{Xw}$ is the best approximation to $\mathbf{y}$ in the range of $\mathbf{X}$. Recall from note 1 that $\mathbf{X}$ is an $n\times d$ matrix, $\mathbf{y}$ is an $n$-dimensional vector, and $\mathbf{w}$ is a $d$-dimensional vector. Suppose that $\mathbf{X}$ is \tit{full rank}, that is, $\dim(\text{range}(\mathbf{X})) = d$. In the language of section 2.2, we have that $V = \R^n$, $W = \text{range}(\mathbf{X}) \cong \R^d$, $\mathbf{v} = \mathbf{y}$, and $\mathbf{v}_w = \mathbf{Xw}$. In the coming weeks, we will complete this derivation using special classes of matrices, and then confirm that it behaves as we would expect by developing the class of \tit{projection matrices}.
\end{document}