\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[overload]{empheq}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}

% These two lines are from this StackExchange post: https://tex.stackexchange.com/a/177270
\usepackage{sectsty}
\allsectionsfont{\mdseries}

\title{Homework $12$}
\author{Math 198: Math for Machine Learning}
\date{}

\begin{document}
\maketitle

\noindent
Due Date:  \\
Name: \\
Student ID:

\section*{Instructions for Submission}
Please include your name and student ID at the top of your homework submission. You may submit handwritten solutions or typed ones (\LaTeX\ preferred). If you at any point write code to help you solve a problem, please include your code at the end of the homework assignment, and mark which code goes with which problem. Homework is due by start of lecture on the due date; it may be submitted in-person at lecture or by emailing a PDF to both facilitators.

\section{Ridge Regression}
Consider the linear regression problem in which we seek to fit weights $\mathbf{w}$ given data $\mathbf{X}$, $\mathbf{y}$ and a noise term $\epsilon \sim \mathcal{N}(0, \sigma^2)$. Suppose we have a prior estimate of our parameters' likelihoods; in particular, we assume $w_i \sim \mathcal{N}(0, c)$. Using everything you have learned in this course, derive the optimal values for $\mathbf{w}$ in terms of $\mathbf{X, y}, \sigma^2$, and $c$. \\
{\color{blue}
We seek to maximize the log-likelihood $$\log\mathcal{L}(\mathbf{w}, \sigma^2) = \log p(\mathbf{w}) + \sum_{i=1}^n \log p(y_i | \mathbf{x_i}; \mathbf{w}, \sigma^2)$$
We first calculate $\log p(\mathbf{w})$:
\begin{align*}
\log p(\mathbf{w}) &= \log \prod_{i=1}^d p(w_i) \\
&= \log\prod_{i=1}^d \frac{1}{\sqrt{2\pi c}}\text{exp}(-\frac{w_i^2}{2c}) \\
&= \log[(\frac{1}{\sqrt{2\pi c}})^d\text{exp}(-\frac{1}{2c}\sum_{i=1}^d w_i^2)] \\
&= -d\log \sqrt{2\pi c} - \frac{1}{2c}\sum_{i=1}^d w_i^2 \\
\end{align*}
Recall from note 12 that $$\sum_{i=1}^n \log p(y_i | \mathbf{x_i}; \mathbf{w}, \sigma^2) = -(\frac{n}{2}\log 2\pi + n\log\sigma + \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \mathbf{x_i^\top w})^2)$$
So we seek to minimize the negative log likelihood $$-\log\mathcal{L}(\mathbf{w}, \sigma^2) = d\log\sqrt{2\pi c} + \frac{1}{2c}\sum_{i=1}^d w_i^2 + \frac{n}{2}\log 2\pi + n\log\sigma + \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \mathbf{x_i\top w})^2$$
Removing the terms which are only constants, we see that this is equivalent to minimizing $$\frac{1}{2c}\sum_{i=1}^d w_i^2 + \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \mathbf{x_i^\top w})^2 = \frac{1}{2c}||\mathbf{w}||_2^2 + \frac{1}{2\sigma^2}||\mathbf{y - Xw}||_2^2$$
For simplicity, we will multiply this through by $2\sigma^2$ (this will not change the optimal $\mathbf{w}$) and let $\lambda = \frac{\sigma^2}{c}$, giving us $$\hat{\mathbf{w}} = \min_{w}[(\mathbf{y - Xw})^\top(\mathbf{y-Xw}) + \lambda\mathbf{w^\top w}]$$
Using matrix calculus, we can then derive 
\begin{align*}
0 &= -2\mathbf{X}^\top(\mathbf{y - X\hat{w}}) + 2\lambda\mathbf{\hat{w}} \\
0 &= -\mathbf{X^\top y} + \mathbf{X^\top X\hat{w}} + \lambda\hat{\mathbf{w}} \\
\mathbf{X^\top y} &= \mathbf{X^\top X\hat{w}} + \lambda\mathbf{\hat{w}} \\
\mathbf{\hat{w}} &= (\mathbf{X^\top X} + \lambda\mathbf{I})^{-1}\mathbf{X^\top y}
\end{align*}
which matches the formulation we present in note 6, with $\lambda = \frac{\sigma^2}{c}$.
}

\end{document}
