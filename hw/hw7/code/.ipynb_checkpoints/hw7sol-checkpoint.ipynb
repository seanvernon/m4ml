{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 198 Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Run this cell to ensure you have the required packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Run this cell to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "from loaders import MNISTLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Run this cell to load the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = MNISTLoader.train_images()\n",
    "train_labels = MNISTLoader.train_labels()\n",
    "input_dim = len(train_images[0]) # The dimension of our input vectors\n",
    "output_dim = 10 # The number of possible labels, 0 through 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Data\n",
    "Run this cell to view a random sample image from the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.random.randint(len(train_images))\n",
    "label = np.argmax(train_labels[num])\n",
    "image = train_images[num].reshape((28, 28))\n",
    "\n",
    "print(\"Label: \" + str(label))\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Code\n",
    "The ```MNIST_Net``` class defines the structure of the neural network we will train. You will need to fill in a few lines of the ```backward``` function, which computes the gradient of the loss function with respect to the weights for each set of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net:\n",
    "    \n",
    "    # input_dim: The dimension of the input vectors\n",
    "    # hidden_dim: The dimension of the hidden layer\n",
    "    # output_dim: The dimension of the output layer (i.e. the number of classes)\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        # Randomly initializes the weight layers with mean 0 and standard deviation .001, \n",
    "        # and initializes the bias layers to all zeroes.\n",
    "        self.V = np.random.normal(0, 1e-3, hidden_dim * input_dim).reshape((hidden_dim, input_dim))\n",
    "        self.b = np.zeros((hidden_dim, 1))\n",
    "        self.W = np.random.normal(0, 1e-3, output_dim * hidden_dim).reshape((output_dim, hidden_dim))\n",
    "        self.c = np.zeros((output_dim, 1))\n",
    "        # Defines the ReLU function to set all negative entries in the input to 0\n",
    "        self.ReLU = lambda x: np.where(x < 0, np.zeros_like(x), x)\n",
    "\n",
    "    # Generates a vector of predictions for the input x\n",
    "    def forward(self, x):\n",
    "        # Apply the first layer of weights to the input\n",
    "        h = (self.V @ x) + self.b\n",
    "        # Apply ReLU\n",
    "        h_prime = self.ReLU(h)\n",
    "        # Apply the second layer of weights\n",
    "        y_prime = (self.W @ h_prime) + self.c\n",
    "        # Save intermediate values\n",
    "        cache = (x, h, h_prime)\n",
    "        return (y_prime, cache)\n",
    "\n",
    "    # Calculates the loss for this prediction\n",
    "    def loss(self, y, y_prime):\n",
    "        return np.linalg.norm(y - y_prime) ** 2\n",
    "    \n",
    "    # Calculates the gradients for each weight and bias layer w.r.t loss\n",
    "    # This function is called \"backward\" because it calculates gradients in reverse order.\n",
    "    # This is done so all calculations are matrix-vector products rather than matrix-matrix;\n",
    "    # this algorithm is known as \"backpropagation\".\n",
    "    def backward(self, y, y_prime, cache):\n",
    "        x, h, h_prime = cache\n",
    "        dy_prime = 2 * (y_prime - y)  # answer to 3.1\n",
    "        dW = dy_prime @ h_prime.T  # answer to 3.2\n",
    "        dc = dy_prime  # answer to 3.3\n",
    "        dh_prime = self.W.T @ dy_prime  # answer to 3.4\n",
    "        dh = np.diag(np.where(h_prime > 0, np.ones_like(h_prime), np.zeros_like(h_prime)).flatten()) @ dh_prime  # answer to 3.5\n",
    "        dV = dh @ x.T  # answer to 3.6\n",
    "        db = dh  # answer to 3.7\n",
    "        return (dV, db, dW, dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the MNIST_Net on a random sample image to make sure everything was initialized correctly\n",
    "# Note: This prediction will probably be wrong, as we have not begun to train the net yet\n",
    "\n",
    "model = MNIST_Net(input_dim, 100, output_dim)\n",
    "\n",
    "num = np.random.randint(len(train_images))\n",
    "image = train_images[num]\n",
    "label = train_labels[num]\n",
    "pred, cache = model.forward(image)\n",
    "\n",
    "print(\"Predicted: {}, Actual: {}, Loss: {}\".format(np.argmax(pred), np.argmax(label), model.loss(label, pred)))\n",
    "display(Image.fromarray(image.reshape((28, 28))))\n",
    "\n",
    "dV, db, dW, dc = model.backward(label, pred, cache)\n",
    "assert dV.shape == model.V.shape, \"dV has incorrect shape\"\n",
    "assert db.shape == model.b.shape, \"db has incorrect shape\"\n",
    "assert dW.shape == model.W.shape, \"dW has incorrect shape\"\n",
    "assert dc.shape == model.c.shape, \"dc has incorrect shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "The ```MNIST_Trainer``` class takes in an ```MNIST_Net``` and training data, trains the net for the given number of epochs, and outputs the training accuracy after each epoch. You will need to fill in one line of the ```train``` function, which performs the weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Trainer:\n",
    "    def __init__(self, model, train_images, train_labels):\n",
    "        self.model = model\n",
    "        self.train_images = train_images\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def train(self, learning_rate=1e-5, num_epochs=5):\n",
    "        for epoch in range(num_epochs):\n",
    "            num_guesses = 0\n",
    "            num_correct = 0\n",
    "\n",
    "            for x, y in zip(self.train_images, self.train_labels):\n",
    "                num_guesses += 1\n",
    "                y_prime, cache = self.model.forward(x)\n",
    "                if np.argmax(y_prime) == np.argmax(y):\n",
    "                    num_correct += 1\n",
    "                dV, db, dW, dc = self.model.backward(y, y_prime, cache)\n",
    "    \n",
    "                # perform weight updates\n",
    "                self.model.V -= (dV * learning_rate)\n",
    "                self.model.b -= (db * learning_rate)\n",
    "                self.model.W -= (dW * learning_rate)\n",
    "                self.model.c -= (dc * learning_rate)\n",
    "            print(\"Training accuracy {}% after {} epochs\".format(round((num_correct / num_guesses) * 100, 2), epoch + 1))\n",
    "\n",
    "    def validate(self, test_images, test_labels):\n",
    "        num_guesses = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        for x, y in zip(test_images, test_labels):\n",
    "            num_guesses += 1\n",
    "            y_prime, cache = self.model.forward(x)\n",
    "            if np.argmax(y_prime) == np.argmax(y):\n",
    "                num_correct += 1\n",
    "        print(\"Test accuracy {}%\".format(round((num_correct / num_guesses) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_Net(input_dim, 100, output_dim)\n",
    "trainer = MNIST_Trainer(model, train_images, train_labels)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = MNISTLoader.test_images()\n",
    "test_labels = MNISTLoader.test_labels()\n",
    "\n",
    "# Moment of truth: will your trained model label test data correctly?\n",
    "num = np.random.randint(len(test_images))\n",
    "image = test_images[num]\n",
    "label = test_labels[num]\n",
    "pred, cache = model.forward(image)\n",
    "\n",
    "print(\"Predicted: {}, Actual: {}, Loss: {}\".format(np.argmax(pred), np.argmax(label), model.loss(label, pred)))\n",
    "display(Image.fromarray(image.reshape((28, 28))))\n",
    "trainer.validate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
